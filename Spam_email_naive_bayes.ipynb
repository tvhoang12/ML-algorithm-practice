{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VFa4dnaK-Xo"
      },
      "outputs": [],
      "source": [
        "#import lib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import os\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import string\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# download data"
      ],
      "metadata": {
        "id": "YeVbt3ElLsyS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/drive') #mount drive first then read data from Colab\n",
        "\n",
        "for dirname, _, filenames in os.walk('/drive/MyDrive/Colab Notebooks'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "df = pd.read_csv(\"/drive/MyDrive/Colab Notebooks/emails.csv\")\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "0i_00jysLvbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#drop duplicate row\n",
        "print(f\"number of duplicate: {df.duplicated().sum()}\")\n",
        "\n",
        "df.drop_duplicates(inplace=True)\n",
        "print(f\"Successfully deleted duplicates {df.shape}\")\n",
        "\n",
        "#check data\n",
        "df['spam'].value_counts()"
      ],
      "metadata": {
        "id": "N_S1rFW2Pnzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#visualize data\n",
        "# Count the number of spam and non-spam emails\n",
        "spam_count = df['spam'].value_counts()[1]\n",
        "non_spam_count = df['spam'].value_counts()[0]\n",
        "\n",
        "# Create a bar chart\n",
        "plt.bar(['Spam', 'Not Spam'], [spam_count, non_spam_count])\n",
        "plt.title('Spam vs Not Spam')\n",
        "plt.xlabel('Email Type')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R5RhwgVWQ67C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download packages\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "gOaIBvdiSDWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "randomSample = df[\"text\"].sample(3)\n",
        "for text in randomSample:\n",
        "  print(text,'\\n')"
      ],
      "metadata": {
        "id": "UcclGSz6SZOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing function\n",
        "def preprocessing_text(text):\n",
        "  # Lowercase the text\n",
        "  text = text.lower()\n",
        "\n",
        "  # Tokenize the text\n",
        "  tokens = word_tokenize(text)\n",
        "\n",
        "  # Remove punctuation and non-alphanumeric characters\n",
        "  tokens = [word for word in tokens if word.isalnum()]\n",
        "\n",
        "  # Remove stopwords\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "  # Initialize steming\n",
        "  stemmer = PorterStemmer()\n",
        "  tokens = [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "  # Join tokens back into a processed text\n",
        "  processed_text = ' '.join(tokens)\n",
        "\n",
        "  return processed_text\n",
        "\n",
        "# Download punkt_tab\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Assign a new column with processed text\n",
        "df['processed_text'] = df['text'].apply(preprocessing_text)\n",
        "df['processed_text'].sample(5)"
      ],
      "metadata": {
        "id": "qr9DeOgzUsr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_text = df['processed_text']\n",
        "y = df['spam']\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_text, y,\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=42)"
      ],
      "metadata": {
        "id": "3r4yK8BSWvIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spam_counts = [y_train.value_counts()[1], y_test.value_counts()[1]]\n",
        "non_spam_counts = [y_train.value_counts()[0], y_test.value_counts()[0]]\n",
        "\n",
        "x_labels = [\"Train\", \"Test\"]\n",
        "\n",
        "plt.bar(x_labels, spam_counts, label='Spam')\n",
        "plt.bar(x_labels, non_spam_counts, bottom=spam_counts, label='Not Spam')\n",
        "plt.title('Spam vs Not Spam in Train and Test Sets')\n",
        "plt.xlabel('Dataset')\n",
        "plt.ylabel('Count')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UPFA5rlPXGFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorize = CountVectorizer()#vectorize the data\n",
        "x_train_vectorized = vectorize.fit_transform(x_train)\n",
        "x_test_vectorized = vectorize.transform(x_test)\n",
        "\n",
        "print(f\"X_train_vec: {x_train_vectorized.toarray().shape}\")\n",
        "print(f\"X_test_vec: {x_test_vectorized.toarray().shape}\")"
      ],
      "metadata": {
        "id": "BKDLaLJZZItU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "id": "LTF1Q5-dZqfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert data to pytorch tensor\n",
        "X_train_tensor = torch.tensor(x_train_vectorized.toarray(),\n",
        "                             dtype=torch.float32,\n",
        "                             device=device)\n",
        "y_train_tensor = torch.tensor(y_train.values,\n",
        "                             dtype=torch.float32,\n",
        "                             device=device)\n",
        "X_test_tensor = torch.tensor(x_test_vectorized.toarray(),\n",
        "                             dtype=torch.float32,\n",
        "                             device=device)\n",
        "y_test_tensor = torch.tensor(y_test.values,\n",
        "                             dtype=torch.float32,\n",
        "                             device=device)"
      ],
      "metadata": {
        "id": "9JaiQEIlZ7T1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_set = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_set,\n",
        "                          batch_size=batch_size,\n",
        "                          shuffle=True)\n",
        "test_loader = DataLoader(test_set,\n",
        "                         batch_size=batch_size,\n",
        "                         shuffle=False)"
      ],
      "metadata": {
        "id": "Tn9unYi2dkYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SpamFilter(nn.Module):\n",
        "  def __init__(self, input_size):\n",
        "    super(SpamFilter, self).__init__()\n",
        "\n",
        "    self.layer1 = nn.Linear(input_size, 128)\n",
        "    self.layer2 = nn.Linear(128, 64)\n",
        "    self.layer3 = nn.Linear(64, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = torch.relu(self.layer1(x))\n",
        "    out = torch.relu(self.layer2(out))\n",
        "    out = torch.sigmoid(self.layer3(out))\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "dHGSK_OyeyAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#init model\n",
        "input_size = x_train_vectorized.shape[1]\n",
        "model_spam_filter = SpamFilter(input_size)\n",
        "\n",
        "#loss\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# optimizer\n",
        "optimizer = torch.optim.Adam(params=model_spam_filter.parameters(),\n",
        "                            lr=0.001)"
      ],
      "metadata": {
        "id": "0MeQXMKUjfbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training loop\n",
        "for epoch in range(10):\n",
        "  model_spam_filter.train()  # Set the model to training mode\n",
        "  running_loss = 0.0\n",
        "\n",
        "  for inputs, labels in train_loader:\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      outputs = model_spam_filter(inputs)\n",
        "\n",
        "      loss = criterion(outputs, labels.unsqueeze(1))\n",
        "      loss.backward()\n",
        "\n",
        "      optimizer.step()\n",
        "\n",
        "      running_loss += loss.item()\n",
        "\n",
        "\n",
        "  print(f'Epoch {epoch+1}/{10}, Loss: {running_loss}')"
      ],
      "metadata": {
        "id": "EX28gyXTlgWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_spam_filter.eval()\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.inference_mode():\n",
        "  for input, labels in test_loader:\n",
        "    outputs = model_spam_filter(input)\n",
        "    predicted = (outputs > 0.5).float()\n",
        "\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels.unsqueeze(1)).sum().item()\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f'Accuracy: {accuracy * 100} %')"
      ],
      "metadata": {
        "id": "gjuzg5LXoH7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spam_email = \"\"\"Subject: Urgent: Claim Your Prize Now!\n",
        "\n",
        "Congratulations! You have been selected as the lucky winner of our grand prize giveaway! Claim your prize now by clicking on the link below. Don't miss out on this amazing opportunity!\n",
        "\n",
        "Click here to claim your prize: superlottery@gmail.com\n",
        "\n",
        "Hurry, this offer is only available for a limited time!\n",
        "\n",
        "Best Regards,\n",
        "Spammy Marketing Team\n",
        "\"\"\"\n",
        "\n",
        "non_spam_email = \"\"\"Subject: Meeting Agenda for Tomorrow\n",
        "\n",
        "Hi Team,\n",
        "\n",
        "I hope this email finds you well. I wanted to remind everyone about the meeting scheduled for tomorrow at 10:00 AM. Below is the agenda:\n",
        "\n",
        "1. Review of project milestones\n",
        "2. Discussion on upcoming deadlines\n",
        "3. Any other business\n",
        "\n",
        "Please come prepared with any updates or questions you may have. Looking forward to a productive meeting.\n",
        "\n",
        "Best regards,\n",
        "Mr. Bannerjee\n",
        "\"\"\"\n",
        "def classify_email(email_text):\n",
        "    # Preprocess the email\n",
        "    preprocessed_email = preprocessing_text(email_text)\n",
        "\n",
        "    #transform email\n",
        "    vectorized_email = vectorize.transform([preprocessed_email])\n",
        "\n",
        "    #convert to tensor\n",
        "    email_tensor = torch.tensor(vectorized_email.toarray(),\n",
        "                               dtype=torch.float32,\n",
        "                               device=device)\n",
        "\n",
        "    output = model_spam_filter(email_tensor)\n",
        "\n",
        "    if output > 0.5:\n",
        "        return \"spam\"\n",
        "    else:\n",
        "        return \"not spam\"\n",
        "\n",
        "print(f\"email 1 is : {classify_email(spam_email)}\")\n",
        "print(f\"email 2 is : {classify_email(non_spam_email)}\")"
      ],
      "metadata": {
        "id": "kKRLjz_-o0Ot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving PyTorch Model\n",
        "from pathlib import Path\n",
        "\n",
        "# Create model's directory\n",
        "MODEL_PATH = Path(\"models\")\n",
        "MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Create model save path\n",
        "MODEL_NAME = \"Spam_Classification.pth\"\n",
        "MODEL_SAVE_PATH = MODEL_PATH/MODEL_NAME\n",
        "\n",
        "# Save the model's state dict\n",
        "print(f\"Saving model to {MODEL_SAVE_PATH}\")\n",
        "torch.save(obj=model_spam_filter.state_dict(), f=MODEL_SAVE_PATH)"
      ],
      "metadata": {
        "id": "P2x3-AAKrEKI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}